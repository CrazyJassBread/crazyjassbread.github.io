<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Data Mining Review | CrazyBread&#39; Blog</title>
<meta name="keywords" content="Data Mining">
<meta name="description" content="Data Mining 课程复习笔记
笔记目录

认识数据
数据预处理
朴素贝叶斯分类器
决策树分类
基于规则的分类
回归算法
支持向量机 SVM
模型的评价

这里只记载一些重要的知识点 or 需要死记硬背的定义（sad）
这篇笔记中夹杂了许多个人学习时的吐槽，希望可以缓解诸位的背书负担 🎩
附录：

错题大赏会
考试回忆录


认识数据
这一节非常无趣且都是死记硬背的知识点，主要由以下几部分组成：
基本概念 | 数据统计的方法 | 相似性度量 ｜数据可视化｜ 复习小巧思
part one
数据的基本概念
一句话总结： 数据（总体） &gt; 数据对象（比如一张统计表） &gt; 数据元素（表中的列） &gt; 数据项（每列的具体值）
数据属性
阅读参考书目，感觉这里的数据属性指的是机器学习中数据的特征（比如Titanic数据集中的Age、Sex等）
比较搞人的是这里对数据属性也进行了分类，分为四种

标称属性：感觉这里指的是对数据的命名，比如 fanqi 养的六只猫需要六个不同的名字来区分
二元属性：只有两种取值的标称属性
序数属性：比如大中小，但是不知道大究竟是多少（定性分析）
数值属性：分成区间标度（我身高 180cm 比他高 2 cm）和比率标度（我跑步10km/h 比他快一倍）


‼️
定性属性：标称 &amp; 序数
定量属性：区间 &amp; 比率
part two - 数据统计的基本方法
标准差 $\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2}$">
<meta name="author" content="">
<link rel="canonical" href="https://crazyjassbread.github.io/posts/final-exam-review/data-mining/review/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://crazyjassbread.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://crazyjassbread.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://crazyjassbread.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://crazyjassbread.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://crazyjassbread.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://crazyjassbread.github.io/posts/final-exam-review/data-mining/review/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://crazyjassbread.github.io/posts/final-exam-review/data-mining/review/">
  <meta property="og:site_name" content="CrazyBread&#39; Blog">
  <meta property="og:title" content="Data Mining Review">
  <meta property="og:description" content="Data Mining 课程复习笔记 笔记目录
认识数据 数据预处理 朴素贝叶斯分类器 决策树分类 基于规则的分类 回归算法 支持向量机 SVM 模型的评价 这里只记载一些重要的知识点 or 需要死记硬背的定义（sad）
这篇笔记中夹杂了许多个人学习时的吐槽，希望可以缓解诸位的背书负担 🎩
附录：
错题大赏会 考试回忆录 认识数据 这一节非常无趣且都是死记硬背的知识点，主要由以下几部分组成：
基本概念 | 数据统计的方法 | 相似性度量 ｜数据可视化｜ 复习小巧思
part one 数据的基本概念
一句话总结： 数据（总体） &gt; 数据对象（比如一张统计表） &gt; 数据元素（表中的列） &gt; 数据项（每列的具体值）
数据属性
阅读参考书目，感觉这里的数据属性指的是机器学习中数据的特征（比如Titanic数据集中的Age、Sex等）
比较搞人的是这里对数据属性也进行了分类，分为四种
标称属性：感觉这里指的是对数据的命名，比如 fanqi 养的六只猫需要六个不同的名字来区分 二元属性：只有两种取值的标称属性 序数属性：比如大中小，但是不知道大究竟是多少（定性分析） 数值属性：分成区间标度（我身高 180cm 比他高 2 cm）和比率标度（我跑步10km/h 比他快一倍） ‼️ 定性属性：标称 &amp; 序数 定量属性：区间 &amp; 比率
part two - 数据统计的基本方法 标准差 $\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2}$">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-27T16:25:31+08:00">
    <meta property="article:modified_time" content="2025-12-27T16:25:31+08:00">
    <meta property="article:tag" content="Data Mining">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Data Mining Review">
<meta name="twitter:description" content="Data Mining 课程复习笔记
笔记目录

认识数据
数据预处理
朴素贝叶斯分类器
决策树分类
基于规则的分类
回归算法
支持向量机 SVM
模型的评价

这里只记载一些重要的知识点 or 需要死记硬背的定义（sad）
这篇笔记中夹杂了许多个人学习时的吐槽，希望可以缓解诸位的背书负担 🎩
附录：

错题大赏会
考试回忆录


认识数据
这一节非常无趣且都是死记硬背的知识点，主要由以下几部分组成：
基本概念 | 数据统计的方法 | 相似性度量 ｜数据可视化｜ 复习小巧思
part one
数据的基本概念
一句话总结： 数据（总体） &gt; 数据对象（比如一张统计表） &gt; 数据元素（表中的列） &gt; 数据项（每列的具体值）
数据属性
阅读参考书目，感觉这里的数据属性指的是机器学习中数据的特征（比如Titanic数据集中的Age、Sex等）
比较搞人的是这里对数据属性也进行了分类，分为四种

标称属性：感觉这里指的是对数据的命名，比如 fanqi 养的六只猫需要六个不同的名字来区分
二元属性：只有两种取值的标称属性
序数属性：比如大中小，但是不知道大究竟是多少（定性分析）
数值属性：分成区间标度（我身高 180cm 比他高 2 cm）和比率标度（我跑步10km/h 比他快一倍）


‼️
定性属性：标称 &amp; 序数
定量属性：区间 &amp; 比率
part two - 数据统计的基本方法
标准差 $\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2}$">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://crazyjassbread.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Data Mining Review",
      "item": "https://crazyjassbread.github.io/posts/final-exam-review/data-mining/review/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Data Mining Review",
  "name": "Data Mining Review",
  "description": "Data Mining 课程复习笔记 笔记目录\n认识数据 数据预处理 朴素贝叶斯分类器 决策树分类 基于规则的分类 回归算法 支持向量机 SVM 模型的评价 这里只记载一些重要的知识点 or 需要死记硬背的定义（sad）\n这篇笔记中夹杂了许多个人学习时的吐槽，希望可以缓解诸位的背书负担 🎩\n附录：\n错题大赏会 考试回忆录 认识数据 这一节非常无趣且都是死记硬背的知识点，主要由以下几部分组成：\n基本概念 | 数据统计的方法 | 相似性度量 ｜数据可视化｜ 复习小巧思\npart one 数据的基本概念\n一句话总结： 数据（总体） \u0026gt; 数据对象（比如一张统计表） \u0026gt; 数据元素（表中的列） \u0026gt; 数据项（每列的具体值）\n数据属性\n阅读参考书目，感觉这里的数据属性指的是机器学习中数据的特征（比如Titanic数据集中的Age、Sex等）\n比较搞人的是这里对数据属性也进行了分类，分为四种\n标称属性：感觉这里指的是对数据的命名，比如 fanqi 养的六只猫需要六个不同的名字来区分 二元属性：只有两种取值的标称属性 序数属性：比如大中小，但是不知道大究竟是多少（定性分析） 数值属性：分成区间标度（我身高 180cm 比他高 2 cm）和比率标度（我跑步10km/h 比他快一倍） ‼️ 定性属性：标称 \u0026amp; 序数 定量属性：区间 \u0026amp; 比率\npart two - 数据统计的基本方法 标准差 $\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2}$\n",
  "keywords": [
    "Data Mining"
  ],
  "articleBody": "Data Mining 课程复习笔记 笔记目录\n认识数据 数据预处理 朴素贝叶斯分类器 决策树分类 基于规则的分类 回归算法 支持向量机 SVM 模型的评价 这里只记载一些重要的知识点 or 需要死记硬背的定义（sad）\n这篇笔记中夹杂了许多个人学习时的吐槽，希望可以缓解诸位的背书负担 🎩\n附录：\n错题大赏会 考试回忆录 认识数据 这一节非常无趣且都是死记硬背的知识点，主要由以下几部分组成：\n基本概念 | 数据统计的方法 | 相似性度量 ｜数据可视化｜ 复习小巧思\npart one 数据的基本概念\n一句话总结： 数据（总体） \u003e 数据对象（比如一张统计表） \u003e 数据元素（表中的列） \u003e 数据项（每列的具体值）\n数据属性\n阅读参考书目，感觉这里的数据属性指的是机器学习中数据的特征（比如Titanic数据集中的Age、Sex等）\n比较搞人的是这里对数据属性也进行了分类，分为四种\n标称属性：感觉这里指的是对数据的命名，比如 fanqi 养的六只猫需要六个不同的名字来区分 二元属性：只有两种取值的标称属性 序数属性：比如大中小，但是不知道大究竟是多少（定性分析） 数值属性：分成区间标度（我身高 180cm 比他高 2 cm）和比率标度（我跑步10km/h 比他快一倍） ‼️ 定性属性：标称 \u0026 序数 定量属性：区间 \u0026 比率\npart two - 数据统计的基本方法 标准差 $\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2}$\n差异系数 $CV = \\frac{\\sigma}{mean(x)}$\n四分位极差 (range)\n$IQR = Q_3 - Q_1$\n$\\max = Q_3 + 1.5 IQR$ 而 $\\min = Q_1 - 1.5 IQR$\n‼️ 这里的 max 和 min 不是数据集中的最大最小值，而是用来判断离群点的阈值\n离群点检测 除了上述计算方法外还有 LOF 方法，越大（ \u003e1 之后）越离群\n偏度系数 $SK = \\frac{(mean - median)}{\\sigma}$\n峰度系数 $K = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - mean}{\\sigma} \\right)^4$ （正太分布的峰度系数为3）\npart three - 数据相似性度量 标称属性的近邻性度量： $$ d(i,j) = \\frac{p - m}{p} $$ 其中 p 为属性总数，m 为两个对象在属性上取值相同的个数\n二元属性的计算则可以画表来表示，令q r s t 分别表示如下四种情况的个数：\nObject j = 1 Object j = 0 Object i = 1 q r Object i = 0 s t 常用的距离计算方式： $d(i,j) = \\frac{r + s}{q + r + s + t}$\n而倘若我们只关心两个对象同时为1的情况，则变为 $d(i,j) = \\frac{r + s}{q + r + s}$ （理解起来也比较简单，比如疾病检测我们只关心阳性 😼）\n补充：Jaccard系数： $sim(i,j) = \\frac{q}{q + r + s}$\n序数属性的近邻性计算（说成人话就是数据的各种距离度量）\n(1) 闵可夫斯基距离 $$ d(i,j) = \\left( \\sum_{k=1}^{n} |x_{ik} - x_{jk}|^p \\right)^{\\frac{1}{p}} $$根据 p 的不同取值，可以得到不同的距离计算方式\n当 $p = 1$ 时，为曼哈顿距离 当 $p = 2$ 时，为欧氏距离 当 $p \\to \\infty$ 时，为切比雪夫距离 对于距离度量，他们都满足三种性质：非负性、对称性、三角不等式\n(2) 余弦相似性 $$ sim(i,j) = \\frac{\\sum_{k=1}^{n} x_{ik} \\cdot x_{jk}}{\\sqrt{\\sum_{k=1}^{n} x_{ik}^2} \\cdot \\sqrt{\\sum_{k=1}^{n} x_{jk}^2}} = \\frac{x_i \\cdot x_j}{||x_i|| \\cdot ||x_j||} $$part four - 数据可视化 书本中介绍了三个可视化方法，分别是 箱线图可视化、直方图可视化、散点图可视化\n小结 💡 这一部分需要记住：数据的四种类型、四分位极差和那个神奇的 min、max 计算方式，还有一些新引入的概念如偏度、峰度系数\n数据预处理 这一节主要包含以下四个部分：\n数据清洗 | 数据集成 | 数据规约 | 数据转换\n进行数据预处理，主要目的是提升数据的质量，书中给了数据质量的衡量标准：准确性、完整性、一致性、合时性、可信性、可理解性等\n数据清洗 缺失值处理方法：忽略元组、手动填写缺失值、自动填写（各种方法都可以）\n噪声数据：离群点分析（这里就涉及到上一节里算出来的 max 和 min 了）、回归方法\n数据不一致：开动我们的小脑瓜\n数据集成 应该是为了缓解数据冗余，数据集成做的事情是将多个数据源进行合并\n相关性分析\n离散变量：chi-square 统计量： $$\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(o_{ij} - e_{ij})^2}{e_{ij}}$$ Tip : 上述表达式中，o 为观测值，e 为期望值 expected 的计算可以采用变量独立性假设来计算联合概率分布 $e^{ij} = \\frac{(row_i) \\times (column_j)}{total}$\n连续变量：皮尔逊相关系数 $$r_{A,B} = \\frac{\\sum_{i=1}^{n} (a_i - \\overline{A})(b_i - \\overline{B})}{（n-1） \\sigma_A \\sigma_B}$$根据 r 的取值划分成三级：$|r| \u003c 0.4$ 为低度线性相关，$0.4 \\leq |r| \\leq 0.7$ 为显著性相关，$|r| \u003e 0.7$ 为高度线性相关\n补充：相关系数矩阵的计算 $$ R = \\begin{bmatrix} 1 \u0026 r_{1,2} \u0026 \\cdots \u0026 r_{1,n} \\\\ r_{2,1} \u0026 1 \u0026 \\cdots \u0026 r_{2,n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ r_{n,1} \u0026 r_{n,2} \u0026 \\cdots \u0026 1 \\end{bmatrix} $$ 其中 $$r_{i,j} = \\frac{\\sum_{k=1}^{n} (x_{k,i} - \\overline{x_i})(x_{k,j} - \\overline{x_j})}{n \\cdot \\sigma_i \\sigma_j}$$协方差分析 $$cov(A,B) = \\frac{\\sum_{i=1}^{n} (a_i - \\overline{A})(b_i - \\overline{B})}{n}$$Tip : 协方差为 0 不一定代表两个变量是独立的（除非加上如服从正态分布等假设条件）\n数据规约（人话版：如何节省存储空间） 思路一：数据降维（主成分分析PCA大法）\n思路二：降数据（抽样法）\n抽样法：简单随机抽样（SRS，又分成有放回与无放回）、分层抽样等 思路三：数据压缩\n数据转换 数据转换就两种类型，一类规范化，另一类就是离散化\n规范化\n最小-最大规范化 $$v' = \\frac{v - min_A}{max_A - min_A} (new\\_{max_A} - new_{min_A}) + new_{min_A}$$ Z-Score 规范化 $$v' = \\frac{v - \\overline{A}}{\\sigma_A}$$ 小数定标规范化 $$v' = \\frac{v}{10^j}$$ 其中 j 是使用该方法后，$|v'| \u003c 1$ 的最小整数 离散化\n💡 题目中经常把等宽、等频叫做 “分箱”\n等宽法： 将数据划分成宽度相等的区间（工资划分为低（0-3）、中（3-6）、高（6-9）） 等频法： 将数据划分成频率相等的区间（每个区间包含相同的数据点） 聚类：k-means等 ‼️ 上面提到的三种方法都是非监督方法\n朴素贝叶斯分类器 这部分内容构成方式为: 基本概念 | 例题 ｜ 总结\n‼️ 贝叶斯分类器的目标是 利用似然概率和先验概率 预测后验概率\n基本概念 这一部分的核心就两点，第一点是贝叶斯定理 $$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $$我们的目标是在给定数据分布 D 的条件下，找出最有可能的假设 h，即 $$ h_{MAP} = \\arg\\max_{h \\in H} \\frac{P(D|h) P(h)}{P(D)} = \\arg\\max_{h \\in H} P(D|h) P(h) $$第二个比较重要的就是 独立性假设 $$ h_{MAP} = \\max_{h \\in H} \\Pi_{i=1}^{n} P(d_i | h) P(h) $$例题 这篇文章举的例子就是一个不错的例题 😼\n带你理解朴素贝叶斯分类算法 - 忆臻的文章 - 知乎\n除此之外，参考书籍上还由 电脑购买、垃圾邮件 判断的例题\n总结 贝叶斯分类器的特点：（这里是直接照搬的书籍）\n属性可以离散也可以连续 数学基础坚实，分类效率稳定 对缺失和噪声数据不太敏感 属性如果不相关，分类效果很好 决策树分类 这一章介绍了四类决策树算法，决策树可以看作是基于规则的分类器\nHunt 算法 这种算法不需要考虑划分节点的信息属性，只需要递归的添加划分节点，直到所有叶节点都是单一类别为止\n这牵扯到两个问题\n怎样为不同类型的属性指定测试条件 怎样选择最佳划分 下面三种算法就是为了解决第二个问题二提出的\nID3 算法 ID3 算法使用信息增益作为划分属性的选择标准 $$\\begin{aligned} Entropy(D) \u0026= - \\sum_{i=1}^{m} p_i \\log_2(p_i) \\\\ Entropy_A(D) \u0026= \\sum_{j=1}^{v} \\frac{|D_j|}{|D|} \\times Entropy(D_j) \\\\ Gain(A) \u0026= Entropy(D) - Entropy_A(D) \\end{aligned}$$C4.5 算法 C4.5 算法使用信息增益率作为划分属性的选择标准 $$\\begin{aligned} SplitInfo_A(D) \u0026= - \\sum_{j=1}^{v} \\frac{|D_j|}{|D|} \\log_2 \\left( \\frac{|D_j|}{|D|} \\right) \\\\ GainRatio(A) \u0026= \\frac{Gain(A)}{SplitInfo_A(D)} \\end{aligned}$$CART 算法 CART 算法使用基尼指数作为划分属性的选择标准 $$\\begin{aligned} Gini(D) \u0026= 1 - \\sum_{i=1}^{m} p_i^2 \\\\ Gini_A(D) \u0026= \\sum_{j=1}^{v} \\frac{|D_j|}{|D|} \\times Gini(D_j) \\\\ \\Delta Gini(A) \u0026= Gini(D) - Gini_A(D) \\end{aligned}$$ 决策树算法防止过拟合可以通过预剪枝和后剪枝两种方式实现\n基于规则的分类 $$ (Condition) \\to (Class) $$ 在介绍如何制定划分规则之前，先对规则的评估方法进行介绍\n规则的评估\n覆盖率 准确率 根据这两种规则的指标，我们可以制定出规则的优先级\n选择规则排序时，第一种方案是按照规则的质量进行排序，第二种方案是按照规则所属的类别进行排序\n构造规则集的一个最基础的方法是顺序覆盖 + 删除实例\n为什么删除实例：避免规则重复 为什么删除正实例：防止高估后面规则的准确率，确保下一个规则不同 为什么删除负实例：防止过拟合错误数据集，防止低估下一个规则的准确率\n很奇怪的一点，书本上举的例子，对于企鹅这反例的删除位置很古怪，目前还没看明白 Learn One Rule 算法 该算法主要由以下步骤组成\n规则增长：方案一是从一般到特殊，方案二是从特殊到一般\n规则评估：准确率、似然比、Laplace、FOIL\n似然比： $$R = 2 \\sum_{i = 1}^{k} f_i \\log \\left( \\frac{f_i}{e_i} \\right)$$FOIL： $$FOIL = f \\left( \\log_2 \\left( \\frac{f}{f + n} \\right) - \\log_2 \\left( \\frac{F}{F + N} \\right) \\right)$$Laplace： $$Laplace = \\frac{f + 1}{n + k}$$停止条件\n规则剪枝\n💡 这里简单介绍一下 lazy learning 与 eager learning 两种分类方法\n回归算法 本章内容 主要包含以下几部分：\n线性回归 | 最小二乘法 | 梯度下降算法 | 逻辑回归 | 决策树回归\n回归问题是什么，简单来说就是对两个变量之间的关系进行建模\n线性回归 在该问题背景下，我们假设两个变量之间满足最简单的关系——线性关系 $$ y = ax + b $$ 那么分析的目标就是找出“最优”的 a 和 b 值\n最小二乘法 嘿！现在的问题就变成了如何定义这个优了，当然可以使用点到直线的距离衡量，但是更常用的方法是最小二乘法 $$ E(a,b) = \\sum_{i=1}^{n} (y_i - (a x_i + b))^2 $$ 如果我们令$\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ 和 $\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$，那么最优的 a 和 b 可以通过以下公式计算得到： $$ a = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x_i - \\overline{x})^2} = \\frac{\\sum_{i=1}^{n} x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i=1}^{n} x_i^2 -n \\overline{x}^2} $$ $$ b = \\overline{y} - a \\overline{x} $$梯度下降算法 之前求解最优 a 和 b 的方法是通过解析解的方式求解的，除此之外，还可以使用数值解法——梯度下降法\n一些注意事项\n学习率 $\\eta$ 的选择不能太大也不能太小（老生常谈了） 收敛到的结果未必是全局最优值 逻辑回归 逻辑回归个人感觉放在这里怪里怪气，它算是一种分类算法吧（相信书本这么安排肯定有它的道理 🤨）\n逻辑回归的核心思路是，将回归的输出结果转化为 0 - 1 的概率值\n线性回归的原始输出为 $ax + b \\in \\mathbb{R}$，使用 sigmoid 可以将其转化为 0 - 1 之间的概率值 $$ P(y = 1 | x) = \\frac{1}{1 + e^{-(a x + b)}} $$BUT：sigmoid 带来了一个问题，如果仍使用之前的平方损失函数，那么这个结果是非凸的，从而无法使用梯度下降法进行优化\n直觉上告诉我们，对于 $P(y|x)$ 正确分类时肯定越大越好\n那么我们可以构造出 $$ P(y|x) = p_i^{y_i} (1 - p_i)^{1 - y_i} $$ 其中 $p_i = P(y_i = 1 | x_i)$\n为了方便计算，我们对其取对数 $$\\log(P(y|x)) = y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)$$那么简单推导可以得出 $$\\begin{aligned} \\log(P(y|x)) \u0026= y_i (a x_i + b) - \\log(1 + e^{a x_i + b}) \\quad (算作\\ \\ln L)\\\\ \\log \\left( \\frac{p_i}{1 - p_i} \\right) \u0026= a x_i + b \\quad (算作\\ logit) \\end{aligned}$$这个损失函数的最终形式和线性模型一样\nln L 损失函数 使用的是经验风险最小化，不是结构风险最小化，泛化能力差，容易过拟合（摘抄自书后习题）\n优势比 OR（Odds Ratio） 对于上述二分类的情况，优势比就等于 $e^a$\n$$OR = \\frac{(p_1)(1 - p_1)}{(p_0)(1 - p_0)}$$ 其中 $p_1$ 和 $p_0$ 为在第 j 个特征分别取值为 $c_1$ 和 $c_0$ 时属于正类的概率\n对 OR 取对数 $$\\log(OR) = \\log \\left( \\frac{p_1}{1 - p_1} \\right) - \\log \\left( \\frac{p_0}{1 - p_0} \\right) = \\beta_j (c_1 - c_0)$$那么 OR 可以表示为 $$OR = e^{\\beta_j (c_1 - c_0)}$$如果我们令 $c_1 - c_0 = 1$，那么\n$$ OR = e^{\\beta_j} = \\begin{cases} \u003e 1 \u0026 \\beta_j \u003e 0 \\\\ = 1 \u0026 \\beta_j = 0 \\\\ \u003c 1 \u0026 \\beta_j \u003c 0 \\end{cases} $$ 三种情况分别对应 危险因子、无作用、保护因子\n参数估计 对于一个实际发生的样本 $i$，它的概率可以表示为 $$P(y_i | x_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}$$那么似然函数可以表示为 $$ L = \\prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i} $$对似然函数取对数，得到对数似然函数 $$\\begin{aligned} \\log L \u0026= \\sum_{i=1}^{n} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right) \\\\ \u0026= \\sum_{i=1}^{n} \\left( y_i \\log \\frac{p_i}{1 - p_i} - \\log(1 - p_i) \\right) \\\\ \u0026= \\sum_{i=1}^{n} \\left( y_i (a x_i + b) - \\log(1 + e^{a x_i + b}) \\right) \\end{aligned}$$当 $\\log L$ 取到最大值时，逻辑回归的参数估计也就完成了\n正则化 这一部分和正常的机器学习正则化方法类似，主要是为了防止过拟合\nL1 正则化（Lasso 回归）：偏好稀疏编码 L2 正则化（Ridge 回归）：偏向于整体更小 决策树回归 决策树回归的思想比较简单，就是每次将数据所在的空间一分为二，下面的内容也集中在如何选择最优划分节点上了 划分节点的选择很简单，遍历所有的可能划分点，选择使得划分后的均方误差（MSE）最小的点\n很有意思的是，在完成上面一次划分后，可以使用残差 $$r_i = y_i - \\hat{y_i}$$ 来继续进行划分，从而得到更好的拟合效果 (提升树)\n决策树回归相当有趣，可以看课本上的具体例子辅助理解\n支持向量机 SVM SVM 问题的损失函数叫做 Hinge Loss $\\to$ $\\max(0, 1 - y_i(w^Tx_i + b))$\n问题推导：SVM 的目标很简单，就是希望在两个类别之间找到一个最优的划分超平面，先假设数据是线性可分的，而且一个类别为 +1，另一个类别为 -1，那么划分超平面可以表示为 $$\\begin{aligned} \u0026w \\cdot x + b \\\\ \u0026w \\cdot x + b \u003c= -1 \\quad for \\quad y_i = -1 \\\\ \u0026w \\cdot x + b \u003e= 1 \\quad for \\quad y_i = +1 \\end{aligned}$$结合之前学习过的凸优化知识，可以将 SVM 转化为如下所示的优化问题 $$\\begin{aligned} \u0026\\min \\quad \\frac{1}{2} ||w||^2 \\\\ \u0026s.t. \\quad y_i (w \\cdot x_i + b) \u003e= 1, \\quad i = 1, 2, \\ldots, n \\end{aligned}$$求解该优化问题可以使用拉格朗日对偶方法，构造拉格朗日函数 $$L(w, b, \\alpha) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1]$$那么求解该优化问题就变成了求解下面的优化问题 $$\\begin{aligned} \u0026\\min_{w,b} \\max_{\\alpha \u003e= 0} \\left( \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1] \\right) \\\\ = \u0026 \\max_{\\alpha \u003e= 0} \\min_{w,b} \\left( \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (w \\cdot x_i + b) - 1] \\right) \\end{aligned}$$ 上述交换成立的前提是满足 KKT 条件 $$\\begin{aligned} \u0026a_i \\ge 0 \\\\ \u0026y_i (w \\cdot x_i + b) - 1 \\ge 0 \\\\ \u0026\\alpha_i [y_i (w \\cdot x_i + b) - 1] = 0, \\quad i = 1, 2, \\ldots, n \\end{aligned}$$ 通过对拉格朗日函数分别对 w 和 b 求偏导，并令其为 0，可以得到 $$\\begin{aligned} \u0026w = \\sum_{i=1}^{n} \\alpha_i y_i x_i \\\\ \u0026\\sum_{i=1}^{n} \\alpha_i y_i = 0 \\end{aligned}$$最终模型可以表示为 $$f(x) = \\text{sign} \\left( \\sum_{i=1}^{n} \\alpha_i y_i x_i^T \\cdot x + b \\right)$$KKT 条件保证了只有支持向量对应的 $\\alpha_i$ 不为 0，因此最终模型只与支持向量有关\n当然，SVM 还有软间隔和核函数的内容，不过这里就不展开介绍了 😶\n需要知道 Kernel 可以处理非线性可分问题，将数据映射到高维空间\n常见核函数\n高斯 RBF 核函数 $$K(x_i, x_j) = \\exp \\left( - \\frac{||x_i - x_j||^2}{2 \\sigma^2} \\right)$$ 齐次多项式核函数 $$K(x_i, x_j) = (x_i \\cdot x_j)^d$$ 非齐次多项式核函数 $$K(x_i, x_j) = (x_i \\cdot x_j + 1)^d$$ sigmoid 核函数 $$K(x_i, x_j) = \\tanh(k x_i \\cdot x_j + \\theta)$$ 模型的评价 本章一共由三个部分组成： 分类模型评价指标 | 不平衡分类问题 | 过拟合 \u0026 欠拟合\n分类模型评价指标 这一节的核心内容应该就是 ROC 曲线，需要知道 FP FN TP TN 的计算、Precision 和 Recall 以及 F1-score 是什么\nTP：将正类正确分类为正类的数量\nTN：将负类正确分类为负类的数量\nFP：将负类错误分类为正类的数量\nFN：将正类错误分类为负类的数量\n准确率 (Accuracy)：分类器正确分类的比例 $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$精确度 (Precision)：正类预测中真正类所占的比例 $$Precision = \\frac{TP}{TP + FP}$$ 需要注意 Precision 和 Accuracy 二者的不同 ⚠️\n召回率 (Recall)：正类被正确预测的比例 $$Recall = \\frac{TP}{TP + FN}$$F1-score：准确率和召回率的调和平均数 $$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$ROC 曲线 TPR（真正率）和 FPR（假正率）的计算 $$\\begin{aligned} \u0026TPR = \\frac{TP}{TP + FN} \\quad (Recall) \\\\ \u0026FPR = \\frac{FP}{FP + TN} \\end{aligned}$$三种常见情况\n(FPR, TPR) = (0, 0)：将所有样本都预测为负类 (FPR, TPR) = (1, 1)：将所有样本都预测为正类 (FPR, TPR) = (0, 1)：完美分类器 不平衡分类问题 这一块的知识点还蛮重要的，至少在课本后面的题目中经常见到\n核心目标是解决样本数据集不平衡导致分类器偏向于多数类的问题\n第一种方法：重采样，包括过采样和欠采样\n过采样：增加少数类样本数量（可能导致过拟合） 欠采样：减少多数类样本数量（可能训练不充分，但是可以通过多次采样缓解） 第二种方法：两阶段学习，分为特征学习与分类器学习\n两阶段学习：PN-Rules 基于规则的分类 学习分成两个阶段，每个阶段学习一组规则 训练也包含两个阶段 阶段一：学习一组规则，尽可能覆盖正类（少数类） 阶段二：使用阶段一覆盖的正负样本以及少部分为覆盖的负样本，学习一组规则 分类使用两组规则 先使用阶段一的规则进行分类，若分类为负类则输出负类 否则使用阶段二的规则进行分类，若分类为正类则输出正类，否则输出负类 第三种方法：代价敏感学习\n通过调整不同类型分类错误的代价，来缓解不平衡分类问题 代价矩阵 Predicted Positive Predicted Negative Actual Positive 0 $C_{FN}$ Actual Negative $C_{FP}$ 0 F1-score 在不平衡分类问题中更有意义，它更强调精确率和召回率，但不考虑误分类代价\n过拟合 \u0026 欠拟合 过拟合的原因：噪声导致的过拟合、缺乏代表性样本导致的过拟合\n减少泛化误差的方法：添加正则化项、Dropout、交叉验证\n过拟合 欠拟合 低偏差 高偏差 高方差 低方差 关联规则挖掘 内容主要包含以下几部分：\n基本概念 | Apriori算法 | FP-Growth算法\n基本概念 在前面讲述过了规则的定义，那就可以开始学习关联规则挖掘了，如果我们提出假设 $A \\to B$，那么我们需要评估这个假设的质量，常用的评估指标有以下三种： 支持度 (Support)\nAB在一起出现的频率 $$Support(A \\to B) = P(A \\cup B) = \\frac{count(A \\cup B)}{N}$$置信度 (Confidence)\nB 在 A 发生的条件下发生的概率 $$Confidence(A \\to B) = P(B|A) = \\frac{count(A \\cup B)}{count(A)}$$ 提升度 (Lift)\n衡量规则 $A \\to B$ 对 B 发生的影响程度 $$Lift(A \\to B) = \\frac{P(B|A)}{P(B)} = \\frac{Confidence(A \\to B)}{P(B)}$$ ❗️ 记住上面三种评估指标的计算公式\nApriori算法 ⚠️ 候选项集生成的一个重要策略是，在删除最后一个项后，前面的项相同才能合并\n‼️ 如果生成的一个项，他的所有子集有不是频繁的情况，需要剪枝删除\nFP-Growth算法 集成学习 AdaBoost 算法 不断关注前一轮分类失败的样本，将多个弱学习器组合成一个强学习器\n理论上可以训练出误差任意低的分类器，但是弱分类器要求正确率大于 50%\n一般损失函数为指数函数\n具体流程\n初始化样本权重 $$ w_i = \\frac{1}{N}, \\quad i = 1, 2, \\ldots, N $$ 对 m = 1, 2, …, M 重复以下步骤 使用加权样本训练弱分类器 $G_m(x)$ 计算分类误差率 $$ err_m = \\frac{\\sum_{i=1}^{N} w_i I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^{N} w_i} $$ 计算分类器权重 $$ \\alpha_m = \\frac{1}{2} \\log \\left( \\frac{1 - err_m}{err_m} \\right) $$ 更新样本权重 $$ w_i \\leftarrow w_i \\times \\exp(-\\alpha_m y_i G_m(x_i)), \\quad i = 1, 2, \\ldots, N$$ 归一化样本权重 $$ w_i \\leftarrow \\frac{w_i}{\\sum_{j=1}^{N} w_j}, \\quad i = 1, 2, \\ldots, N$$ 最终分类器 $$ G(x) = \\text{sign} \\left( \\sum_{m=1}^{M} \\alpha_m G_m(x) \\right) $$ #TODO\n算一遍课本上的例题 错题大赏会 精选试卷四 1.精选试题四的第三题第二小问，答案给出的对于准确度和召回率的计算非常奇怪，没看明白什么意思\n2.第四题\n计算距离的时候，对于性别籍贯这种名目型属性（课本中叫做标称），如果值相同则距离为0，不同则距离为1 而在计算点到簇的距离时，对于标称数据，计算的是 1 - P，P 代表该值在簇中出现的频率 计算簇到簇的距离时，举个例子 C1 = {男 ： 25 ， ⼥ ：5 ； ⼴州 ： 20 ， 深圳 ：6 ， 韶关 ：4 ； 20 } C2 = {男 ： 3 ， ⼥ ： 12 ； 汕 头 ： 12 ， 深 圳 ： 1 ， 韶 关 ： 2 ； 24 } $$\\begin{aligned} d =\u0026 \\frac{(1 - \\frac{25}{30} \\times \\frac{3}{15}) + (1 - \\frac{5}{30} \\times \\frac{12}{15})}{2} \\\\ \u0026+ \\frac{(1 - \\frac{20}{30} \\times \\frac{0}{15}) + (1 - \\frac{0}{30} \\times \\frac{12}{15}) + (1 - \\frac{6}{30} \\times \\frac{1}{15}) + (1 - \\frac{4}{30} \\times \\frac{2}{15})}{4} \\\\ \u0026+ |20 - 24| \\end{aligned}$$ 精选试卷五 第一题\n关联规则挖掘并不是单纯找出所有满足最小支持度的项，还需要保留那些满足最小置信度的规则 K-means 算法是基于原型 or 质心的分类方法，且需要预先指定 K 值 第二题\n计算贝叶斯分类器，实际上就是计算各个类别的后验概率，然后选择概率最大的类别作为预测结果 题目答案给错了，学生栏的数据统计反了 精选试卷六 单选第七题\n数据预处理包括：数据清洗、数据集成、数据规约、数据变换（感觉这一题更应该选 A 变量代换） 单选第14题\n对于一个包含 n 个项的频繁项集，可以产生 $2^n - 2$ 条关联规则 单选第17题答案错误\n多选第一题\n模式发现不属于预测建模任务，模式匹配更像是数据检索 判断第六题\n特征提取高度依赖特定的领域 判断第十四题\n贝叶斯算法的核心目标是计算后验概率，而且不是依赖全体数据，而是每个类别的条件概率 精选试卷七 单选第十题\n极差在取平均的情况下应该是 36，不知道答案中的31从哪里算出来的 单选第八题\n信息熵的计算方法是 $H = -\\sum_{i=1}^n p_i \\log_2(p_i)$ 在等可能情况下简化成 $H = \\log_2(n)$ 精选试卷八 选择\n第10题\n当训练集规模较大时，SGD 比批处理梯度下降更优 第12题\n逻辑回归对噪声也是敏感的 第十三题\n若 SVM 的支撑向量过多，则说明训练的模型过拟合 多选\n第四题\n对于随机森里的决策树，特征数量 m 越大，单棵树的相关性越大，方差越小；越小则相关性越小但方差变大 第十五题\n属性的别名又叫做维度、特征、字段 精选试卷九 如果对于贝叶斯分类任务中，误把某一个特征多算了一遍，意味着加强了这个特征对分类的影响\n此外如果贝叶斯所有分类都重复一遍，结果可能会发生变化，因为分类的外部概率未知，平方可能会缩小概率差距从而影响分类结果\n决策树的父节点的熵比子节点更大\n对于 噪声的敏感度 排序：\nAdaBoost \u003e 逻辑回归 / 线性 SVM \u003e 软间隔 SVM \u003e 随机森林\nKNN、线性回归需要对数据进行规范化处理，而贝叶斯分类器和决策树不需要（二者关注相对顺序）\n对比 BootStrap 和 K折叠交叉验证\n当数据量小的时候，BootStrap 更合适 BootStrap 对集成学习有很大帮助 交叉验证一般能提高模型的泛化能力 难以划分训练测试集的时候，BootStrap 更合适 虽然决策树也能处理回归问题，但是当需要得到函数表达式时，线性回归更合适\n三种可以处理高维数据可视化的方法\n平行坐标系 散点图矩阵 切尔诺夫脸 关于 One Hot 独热编码 🆚 LabelEncoder 标签编码\n对数值不敏感的模型（如决策树、朴素贝叶斯等），可以使用标签编码 对数值敏感的模型（如线性回归、逻辑回归、KNN、SVM 等），应该使用独热编码 精选试卷十 SVM 对于类别不平衡的分类问题也能有不错的表现，而贝叶斯、神经网络、KNN 等模型则非常敏感\nPCA 的投影方向是数据散度更大的方向，而 LDA 的投影方向才是类别区分度更大的方向（多了一个label的约束）\n此外 PCA 并不强调属性值规范化（附加项），常规流程包含 取均值、矩阵特征值分解、坐标变换\nKernel Trick 的核心思想是通过核函数计算高维空间中数据点的内积，从而避免显式地进行高维映射，降低计算复杂度\n计算统计数据的标准差时，一般使用样本标准差公式（分母为 n - 1），而不是总体标准差公式（分母为 n），但是对数据进行 Z-score 规范化时，使用的标准差是总体标准差公式（分母为 n）\n精选试卷十一 当进行 Lasso 回归时，如果一个特征被放大了 k \u003e 1 倍，那么它对应的系数会缩小 k 倍，反而是的在进行正则化时，对损失函数的贡献变小，更容易保留该特征\n数据质量的指标不包含 精确性(Precision)\n书本上给出的数据质量的指标是\n准确性 完整性 一致性 合时性 可信性、可解释性 对于双峰数据类型，使用离散化方法来处理更合适，而 Min-Max 规范化和 Z-score 规范化更适合单峰数据类型\n逻辑回归比决策树更不容易过拟合\n对于样本不均衡的分类任务，使用 F1-score、G-mean、AUC 作为评价指标更合适，而 Accuracy 则不合适\n精选试卷十二 对于 逻辑回归 而言，MSE 不是一个合适的损失函数\nSVM 与 逻辑回归的核心区别在于 LOSS 函数不同\n考试回忆录 基本全是原题，十二套模拟卷基本完成了测试集的全覆盖\n除了有简答题考了 hunt 决策树算法的基本原则和对标称数据的处理方法之外，其余题目基本都是已有的题目，绝大部分都没有修改数据\n",
  "wordCount" : "1891",
  "inLanguage": "en",
  "datePublished": "2025-12-27T16:25:31+08:00",
  "dateModified": "2025-12-27T16:25:31+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://crazyjassbread.github.io/posts/final-exam-review/data-mining/review/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "CrazyBread' Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://crazyjassbread.github.io/favicon.ico"
    }
  }
}
</script>
     <script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net.cn/npm/mathjax@3/es5/tex-chtml.js"
></script>
<script>
  MathJax = {
    tex: {
      displayMath: [
        ["\\[", "\\]"],
        ["$$", "$$"],
      ], 
      inlineMath: [
        ["\\(", "\\)"],
        ["$", "$"]  
      ],
    },
  }
</script> 
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://crazyjassbread.github.io/" accesskey="h" title="CrazyBread&#39; Blog (Alt + H)">CrazyBread&#39; Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://crazyjassbread.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://crazyjassbread.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://crazyjassbread.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://crazyjassbread.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://crazyjassbread.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://crazyjassbread.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Data Mining Review
    </h1>
    <div class="post-meta"><span title='2025-12-27 16:25:31 +0800 CST'>December 27, 2025</span>&nbsp;·&nbsp;9 min

</div>
  </header> 
  <div class="post-content"><h1 id="data-mining-课程复习笔记">Data Mining 课程复习笔记<a hidden class="anchor" aria-hidden="true" href="#data-mining-课程复习笔记">#</a></h1>
<p>笔记目录</p>
<ul>
<li><a href="#%E8%AE%A4%E8%AF%86%E6%95%B0%E6%8D%AE">认识数据</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">数据预处理</a></li>
<li><a href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">朴素贝叶斯分类器</a></li>
<li><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB">决策树分类</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E5%88%86%E7%B1%BB">基于规则的分类</a></li>
<li><a href="#%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">回归算法</a></li>
<li><a href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-svm">支持向量机 SVM</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BB%B7">模型的评价</a></li>
</ul>
<p>这里只记载一些重要的知识点 or 需要死记硬背的定义（sad）</p>
<p>这篇笔记中夹杂了许多个人学习时的吐槽，希望可以缓解诸位的背书负担 🎩</p>
<p>附录：</p>
<ul>
<li><a href="#%E9%94%99%E9%A2%98%E5%A4%A7%E8%B5%8F%E4%BC%9A">错题大赏会</a></li>
<li><a href="#%E8%80%83%E8%AF%95%E5%9B%9E%E5%BF%86%E5%BD%95">考试回忆录</a></li>
</ul>
<hr>
<h2 id="认识数据">认识数据<a hidden class="anchor" aria-hidden="true" href="#认识数据">#</a></h2>
<p>这一节非常无趣且都是死记硬背的知识点，主要由以下几部分组成：</p>
<p><a href="#part-one">基本概念</a> | <a href="#part-two---%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95">数据统计的方法</a> | <a href="#part-three---%E6%95%B0%E6%8D%AE%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F">相似性度量</a> ｜<a href="#part-four---%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96">数据可视化</a>｜ <a href="#%E5%B0%8F%E7%BB%93">复习小巧思</a></p>
<h3 id="part-one">part one<a hidden class="anchor" aria-hidden="true" href="#part-one">#</a></h3>
<p><strong>数据的基本概念</strong></p>
<p>一句话总结： 数据（总体） &gt; 数据对象（比如一张统计表） &gt; 数据元素（表中的列） &gt; 数据项（每列的具体值）</p>
<p><strong>数据属性</strong></p>
<p>阅读参考书目，感觉这里的数据属性指的是机器学习中数据的特征（比如Titanic数据集中的Age、Sex等）</p>
<p>比较搞人的是这里对数据属性也进行了分类，分为四种</p>
<ul>
<li>标称属性：感觉这里指的是对数据的命名，比如 fanqi 养的六只猫需要六个不同的名字来区分</li>
<li>二元属性：只有两种取值的标称属性</li>
<li>序数属性：比如大中小，但是不知道大究竟是多少（定性分析）</li>
<li>数值属性：分成区间标度（我身高 180cm 比他高 2 cm）和比率标度（我跑步10km/h 比他快一倍）</li>
</ul>
<blockquote>
<p>‼️
定性属性：标称 &amp; 序数
定量属性：区间 &amp; 比率</p></blockquote>
<h3 id="part-two---数据统计的基本方法">part two - 数据统计的基本方法<a hidden class="anchor" aria-hidden="true" href="#part-two---数据统计的基本方法">#</a></h3>
<p><strong>标准差</strong> $\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2}$</p>
<p><strong>差异系数</strong> $CV = \frac{\sigma}{mean(x)}$</p>
<p><strong>四分位极差 (range)</strong></p>
<p>$IQR = Q_3 - Q_1$</p>
<p>$\max = Q_3 + 1.5 IQR$ 而 $\min = Q_1 - 1.5 IQR$</p>
<blockquote>
<p>‼️ 这里的 max 和 min 不是数据集中的最大最小值，而是用来判断离群点的阈值</p></blockquote>
<blockquote>
<p><mark>离群点检测</mark> 除了上述计算方法外还有 LOF 方法，越大（ &gt;1 之后）越离群</p></blockquote>
<p><strong>偏度系数</strong> $SK = \frac{(mean - median)}{\sigma}$</p>
<p><strong>峰度系数</strong> $K = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i - mean}{\sigma} \right)^4$ （正太分布的峰度系数为3）</p>
<h3 id="part-three---数据相似性度量">part three - 数据相似性度量<a hidden class="anchor" aria-hidden="true" href="#part-three---数据相似性度量">#</a></h3>
<p>标称属性的<mark><strong>近邻性</strong></mark>度量：
</p>
$$
d(i,j) = \frac{p - m}{p}
$$<p>
其中 p 为属性总数，m 为两个对象在属性上取值相同的个数</p>
<p>二元属性的计算则可以画表来表示，令q r s t 分别表示如下四种情况的个数：</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Object j = 1</th>
          <th>Object j = 0</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Object i = 1</td>
          <td>q</td>
          <td>r</td>
      </tr>
      <tr>
          <td>Object i = 0</td>
          <td>s</td>
          <td>t</td>
      </tr>
  </tbody>
</table>
<p>常用的距离计算方式： $d(i,j) = \frac{r + s}{q + r + s + t}$</p>
<p>而倘若我们只关心两个对象同时为1的情况，则变为 $d(i,j) = \frac{r + s}{q + r + s}$ （理解起来也比较简单，比如疾病检测我们只关心阳性 😼）</p>
<p>补充：Jaccard系数： $sim(i,j) = \frac{q}{q + r + s}$</p>
<p><strong>序数属性的近邻性计算</strong>（说成人话就是数据的各种距离度量）</p>
<p>(1) 闵可夫斯基距离
</p>
$$
d(i,j) = \left( \sum_{k=1}^{n} |x_{ik} - x_{jk}|^p \right)^{\frac{1}{p}}
$$<p>根据 p 的不同取值，可以得到不同的距离计算方式</p>
<ul>
<li>当 $p = 1$ 时，为曼哈顿距离</li>
<li>当 $p = 2$ 时，为欧氏距离</li>
<li>当 $p \to \infty$ 时，为切比雪夫距离</li>
</ul>
<p>对于距离度量，他们都满足三种性质：非负性、对称性、三角不等式</p>
<p>(2) 余弦相似性
</p>
$$
sim(i,j) = \frac{\sum_{k=1}^{n} x_{ik} \cdot x_{jk}}{\sqrt{\sum_{k=1}^{n} x_{ik}^2} \cdot \sqrt{\sum_{k=1}^{n} x_{jk}^2}} = \frac{x_i \cdot x_j}{||x_i|| \cdot ||x_j||}
$$<h3 id="part-four---数据可视化">part four - 数据可视化<a hidden class="anchor" aria-hidden="true" href="#part-four---数据可视化">#</a></h3>
<p>书本中介绍了三个可视化方法，分别是
箱线图可视化、直方图可视化、散点图可视化</p>
<h3 id="小结">小结<a hidden class="anchor" aria-hidden="true" href="#小结">#</a></h3>
<p>💡 这一部分需要记住：数据的四种类型、四分位极差和那个神奇的 min、max 计算方式，还有一些新引入的概念如偏度、峰度系数</p>
<hr>
<h2 id="数据预处理">数据预处理<a hidden class="anchor" aria-hidden="true" href="#数据预处理">#</a></h2>
<p>这一节主要包含以下四个部分：</p>
<p><a href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97">数据清洗</a> | <a href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90">数据集成</a> | <a href="#%E6%95%B0%E6%8D%AE%E8%A7%84%E7%BA%A6%EF%BC%88%E4%BA%BA%E8%AF%9D%E7%89%88%EF%BC%9A%E5%A6%82%E4%BD%95%E8%8A%82%E7%9C%81%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4%EF%BC%89">数据规约</a> | <a href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2">数据转换</a></p>
<p>进行数据预处理，主要目的是提升数据的质量，书中给了数据质量的衡量标准：准确性、完整性、一致性、合时性、可信性、可理解性等</p>
<h3 id="数据清洗">数据清洗<a hidden class="anchor" aria-hidden="true" href="#数据清洗">#</a></h3>
<p>缺失值处理方法：忽略元组、手动填写缺失值、自动填写（各种方法都可以）</p>
<p>噪声数据：离群点分析（这里就涉及到上一节里算出来的 max 和 min 了）、回归方法</p>
<p>数据不一致：开动我们的小脑瓜</p>
<h3 id="数据集成">数据集成<a hidden class="anchor" aria-hidden="true" href="#数据集成">#</a></h3>
<p>应该是为了缓解数据冗余，数据集成做的事情是将多个数据源进行合并</p>
<p><strong>相关性分析</strong></p>
<p>离散变量：chi-square 统计量：
</p>
$$\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(o_{ij} - e_{ij})^2}{e_{ij}}$$<blockquote>
</blockquote>
<p><mark>Tip</mark> : 上述表达式中，o 为观测值，e 为期望值
expected 的计算可以采用变量独立性假设来计算联合概率分布 $e^{ij} = \frac{(row_i) \times (column_j)}{total}$</p>
<p>连续变量：皮尔逊相关系数
</p>
$$r_{A,B} = \frac{\sum_{i=1}^{n} (a_i - \overline{A})(b_i - \overline{B})}{（n-1） \sigma_A \sigma_B}$$<p>根据 r 的取值划分成三级：$|r| < 0.4$ 为低度线性相关，$0.4 \leq |r| \leq 0.7$ 为显著性相关，$|r| > 0.7$ 为高度线性相关</p>
<p>补充：相关系数矩阵的计算
</p>
$$
R = \begin{bmatrix}
1 & r_{1,2} & \cdots & r_{1,n} \\
r_{2,1} & 1 & \cdots & r_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
r_{n,1} & r_{n,2} & \cdots & 1
\end{bmatrix}
$$<p>
其中 </p>
$$r_{i,j} = \frac{\sum_{k=1}^{n} (x_{k,i} - \overline{x_i})(x_{k,j} - \overline{x_j})}{n \cdot \sigma_i \sigma_j}$$<p><strong>协方差分析</strong>
</p>
$$cov(A,B) = \frac{\sum_{i=1}^{n} (a_i - \overline{A})(b_i - \overline{B})}{n}$$<p><mark>Tip</mark> : 协方差为 0 不一定代表两个变量是独立的（除非加上如服从正态分布等假设条件）</p>
<h3 id="数据规约人话版如何节省存储空间">数据规约（人话版：如何节省存储空间）<a hidden class="anchor" aria-hidden="true" href="#数据规约人话版如何节省存储空间">#</a></h3>
<p>思路一：数据降维（主成分分析PCA大法）</p>
<p>思路二：降数据（抽样法）</p>
<pre><code>抽样法：简单随机抽样（SRS，又分成有放回与无放回）、分层抽样等
</code></pre>
<p>思路三：数据压缩</p>
<h3 id="数据转换">数据转换<a hidden class="anchor" aria-hidden="true" href="#数据转换">#</a></h3>
<p>数据转换就两种类型，一类规范化，另一类就是离散化</p>
<p><strong>规范化</strong></p>
<ul>
<li>最小-最大规范化
$$v' = \frac{v - min_A}{max_A - min_A} (new\_{max_A} - new_{min_A}) + new_{min_A}$$</li>
<li>Z-Score 规范化
$$v' = \frac{v - \overline{A}}{\sigma_A}$$</li>
<li>小数定标规范化
$$v' = \frac{v}{10^j}$$ 其中 j 是使用该方法后，$|v'| < 1$ 的最小整数</li>
</ul>
<p><strong>离散化</strong></p>
<blockquote>
<p>💡 题目中经常把等宽、等频叫做 <mark>“分箱”</mark></p></blockquote>
<ul>
<li>等宽法： 将数据划分成宽度相等的区间（工资划分为低（0-3）、中（3-6）、高（6-9））</li>
<li>等频法： 将数据划分成频率相等的区间（每个区间包含相同的数据点）</li>
<li>聚类：k-means等</li>
</ul>
<p>‼️ 上面提到的三种方法都是非监督方法</p>
<hr>
<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器<a hidden class="anchor" aria-hidden="true" href="#朴素贝叶斯分类器">#</a></h2>
<p>这部分内容构成方式为: <a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">基本概念</a> | <a href="#%E4%BE%8B%E9%A2%98">例题</a> ｜ <a href="#%E6%80%BB%E7%BB%93">总结</a></p>
<blockquote>
<p>‼️ 贝叶斯分类器的目标是 利用<strong>似然概率</strong>和<strong>先验概率</strong> 预测<strong>后验概率</strong></p></blockquote>
<h3 id="基本概念">基本概念<a hidden class="anchor" aria-hidden="true" href="#基本概念">#</a></h3>
<p>这一部分的核心就两点，第一点是<strong>贝叶斯定理</strong>
</p>
$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$<p>我们的目标是在给定数据分布 D 的条件下，找出最有可能的假设 h，即
</p>
$$
h_{MAP} = \arg\max_{h \in H} \frac{P(D|h) P(h)}{P(D)} = \arg\max_{h \in H} P(D|h) P(h)
$$<p>第二个比较重要的就是 <strong>独立性假设</strong>
</p>
$$
h_{MAP} = \max_{h \in H} \Pi_{i=1}^{n} P(d_i | h) P(h)
$$<h3 id="例题">例题<a hidden class="anchor" aria-hidden="true" href="#例题">#</a></h3>
<p>这篇文章举的例子就是一个不错的例题 😼</p>
<p><a href="https://zhuanlan.zhihu.com/p/26262151">带你理解朴素贝叶斯分类算法 - 忆臻的文章 - 知乎</a></p>
<p>除此之外，参考书籍上还由 <mark>电脑购买、垃圾邮件</mark> 判断的例题</p>
<h3 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h3>
<p>贝叶斯分类器的特点：（这里是直接照搬的书籍）</p>
<ul>
<li>属性可以离散也可以连续</li>
<li>数学基础坚实，分类效率稳定</li>
<li>对缺失和噪声数据不太敏感</li>
<li>属性如果不相关，分类效果很好</li>
</ul>
<hr>
<h2 id="决策树分类">决策树分类<a hidden class="anchor" aria-hidden="true" href="#决策树分类">#</a></h2>
<p>这一章介绍了四类决策树算法，决策树可以看作是<mark>基于规则</mark>的分类器</p>
<h3 id="hunt-算法">Hunt 算法<a hidden class="anchor" aria-hidden="true" href="#hunt-算法">#</a></h3>
<p>这种算法不需要考虑划分节点的信息属性，只需要递归的添加划分节点，直到所有叶节点都是单一类别为止</p>
<p>这牵扯到两个问题</p>
<ul>
<li>怎样为不同类型的属性指定测试条件</li>
<li>怎样选择最佳划分</li>
</ul>
<p>下面三种算法就是为了解决第二个问题二提出的</p>
<h3 id="id3-算法">ID3 算法<a hidden class="anchor" aria-hidden="true" href="#id3-算法">#</a></h3>
<p>ID3 算法使用信息增益作为划分属性的选择标准
</p>
$$\begin{aligned}
Entropy(D) &= - \sum_{i=1}^{m} p_i \log_2(p_i) \\
Entropy_A(D) &= \sum_{j=1}^{v} \frac{|D_j|}{|D|} \times Entropy(D_j) \\
Gain(A) &= Entropy(D) - Entropy_A(D)
\end{aligned}$$<h3 id="c45-算法">C4.5 算法<a hidden class="anchor" aria-hidden="true" href="#c45-算法">#</a></h3>
<p>C4.5 算法使用信息增益率作为划分属性的选择标准
</p>
$$\begin{aligned}
SplitInfo_A(D) &= - \sum_{j=1}^{v} \frac{|D_j|}{|D|} \log_2 \left( \frac{|D_j|}{|D|} \right) \\
GainRatio(A) &= \frac{Gain(A)}{SplitInfo_A(D)}
\end{aligned}$$<h3 id="cart-算法">CART 算法<a hidden class="anchor" aria-hidden="true" href="#cart-算法">#</a></h3>
<p>CART 算法使用基尼指数作为划分属性的选择标准
</p>
$$\begin{aligned}
Gini(D) &= 1 - \sum_{i=1}^{m} p_i^2 \\
Gini_A(D) &= \sum_{j=1}^{v} \frac{|D_j|}{|D|} \times Gini(D_j) \\
\Delta Gini(A) &= Gini(D) - Gini_A(D)
\end{aligned}$$<blockquote>
<p>决策树算法防止过拟合可以通过预剪枝和后剪枝两种方式实现</p></blockquote>
<hr>
<h2 id="基于规则的分类">基于规则的分类<a hidden class="anchor" aria-hidden="true" href="#基于规则的分类">#</a></h2>
$$
(Condition) \to (Class)
$$<p>
在介绍如何制定划分规则之前，先对规则的评估方法进行介绍</p>
<p><strong>规则的评估</strong></p>
<ul>
<li>覆盖率</li>
<li>准确率</li>
</ul>
<p>根据这两种规则的指标，我们可以制定出规则的优先级</p>
<p>选择规则排序时，第一种方案是按照规则的质量进行排序，第二种方案是按照规则所属的类别进行排序</p>
<p>构造规则集的一个最基础的方法是顺序覆盖 + 删除实例</p>
<blockquote>
<p>为什么删除实例：避免规则重复
为什么删除正实例：防止高估后面规则的准确率，确保下一个规则不同
为什么删除负实例：防止过拟合错误数据集，防止低估下一个规则的准确率</p></blockquote>
<ul>
<li><input disabled="" type="checkbox"> 很奇怪的一点，书本上举的例子，对于企鹅这反例的删除位置很古怪，目前还没看明白</li>
</ul>
<h3 id="learn-one-rule-算法">Learn One Rule 算法<a hidden class="anchor" aria-hidden="true" href="#learn-one-rule-算法">#</a></h3>
<p>该算法主要由以下步骤组成</p>
<p><strong>规则增长</strong>：方案一是从一般到特殊，方案二是从特殊到一般</p>
<p><strong>规则评估</strong>：准确率、似然比、Laplace、FOIL</p>
<p>似然比： </p>
$$R = 2 \sum_{i = 1}^{k} f_i \log \left( \frac{f_i}{e_i} \right)$$<p>FOIL： </p>
$$FOIL = f \left( \log_2 \left( \frac{f}{f + n} \right) - \log_2 \left( \frac{F}{F + N} \right) \right)$$<p>Laplace： </p>
$$Laplace = \frac{f + 1}{n + k}$$<p><strong>停止条件</strong></p>
<p><strong>规则剪枝</strong></p>
<blockquote>
<p>💡 这里简单介绍一下 lazy learning 与 eager learning 两种分类方法</p></blockquote>
<h2 id="回归算法">回归算法<a hidden class="anchor" aria-hidden="true" href="#回归算法">#</a></h2>
<p>本章内容
主要包含以下几部分：</p>
<p><a href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">线性回归</a> | <a href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">最小二乘法</a> | <a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95">梯度下降算法</a> | <a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">逻辑回归</a> | <a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%9B%9E%E5%BD%92">决策树回归</a></p>
<p>回归问题是什么，简单来说就是对两个变量之间的关系进行建模</p>
<h3 id="线性回归">线性回归<a hidden class="anchor" aria-hidden="true" href="#线性回归">#</a></h3>
<p>在该问题背景下，我们假设两个变量之间满足最简单的关系——线性关系
</p>
$$
y = ax + b
$$<p>
那么分析的目标就是找出“最优”的 a 和 b 值</p>
<h3 id="最小二乘法">最小二乘法<a hidden class="anchor" aria-hidden="true" href="#最小二乘法">#</a></h3>
<p>嘿！现在的问题就变成了如何定义这个优了，当然可以使用点到直线的距离衡量，但是更常用的方法是<strong>最小二乘法</strong>
</p>
$$
E(a,b) = \sum_{i=1}^{n} (y_i - (a x_i + b))^2
$$<p>
如果我们令$\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i$ 和 $\overline{y} = \frac{1}{n} \sum_{i=1}^n y_i$，那么最优的 a 和 b 可以通过以下公式计算得到：
</p>
$$
a = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^{n} (x_i - \overline{x})^2} = \frac{\sum_{i=1}^{n} x_i y_i - n \overline{x} \overline{y}}{\sum_{i=1}^{n} x_i^2 -n \overline{x}^2}
$$<p>
</p>
$$
b = \overline{y} - a \overline{x}
$$<h3 id="梯度下降算法">梯度下降算法<a hidden class="anchor" aria-hidden="true" href="#梯度下降算法">#</a></h3>
<p>之前求解最优 a 和 b 的方法是通过解析解的方式求解的，除此之外，还可以使用数值解法——梯度下降法</p>
<p>一些注意事项</p>
<ul>
<li>学习率 $\eta$ 的选择不能太大也不能太小（老生常谈了）</li>
<li>收敛到的结果未必是全局最优值</li>
</ul>
<h3 id="逻辑回归">逻辑回归<a hidden class="anchor" aria-hidden="true" href="#逻辑回归">#</a></h3>
<p>逻辑回归个人感觉放在这里怪里怪气，它算是一种分类算法吧（相信书本这么安排肯定有它的道理 🤨）</p>
<blockquote>
<p>逻辑回归的核心思路是，将回归的输出结果转化为 0 - 1 的概率值</p></blockquote>
<p>线性回归的原始输出为 $ax + b \in \mathbb{R}$，使用 sigmoid 可以将其转化为 0 - 1 之间的概率值
</p>
$$
P(y = 1 | x) = \frac{1}{1 + e^{-(a x + b)}}
$$<p><mark>BUT</mark>：sigmoid 带来了一个问题，如果仍使用之前的平方损失函数，那么这个结果是非凸的，从而无法使用梯度下降法进行优化</p>
<p>直觉上告诉我们，对于 $P(y|x)$ 正确分类时肯定越大越好</p>
<p>那么我们可以构造出
</p>
$$
P(y|x) = p_i^{y_i} (1 - p_i)^{1 - y_i}
$$<p>
其中 $p_i = P(y_i = 1 | x_i)$</p>
<p>为了方便计算，我们对其取对数
</p>
$$\log(P(y|x)) = y_i \log(p_i) + (1 - y_i) \log(1 - p_i)$$<p>那么简单推导可以得出
</p>
$$\begin{aligned}
\log(P(y|x)) &= y_i (a x_i + b) - \log(1 + e^{a x_i + b}) \quad (算作\ \ln L)\\
\log \left( \frac{p_i}{1 - p_i} \right) &= a x_i + b \quad (算作\ logit)
\end{aligned}$$<p>这个损失函数的最终形式和线性模型一样</p>
<blockquote>
<p><mark>ln L 损失函数</mark> 使用的是经验风险最小化，不是结构风险最小化，泛化能力差，容易过拟合（摘抄自书后习题）</p></blockquote>
<h4 id="优势比-orodds-ratio">优势比 OR（Odds Ratio）<a hidden class="anchor" aria-hidden="true" href="#优势比-orodds-ratio">#</a></h4>
<blockquote>
<p>对于上述二分类的情况，优势比就等于 $e^a$</p></blockquote>
$$OR = \frac{(p_1)(1 - p_1)}{(p_0)(1 - p_0)}$$<p>
其中 $p_1$ 和 $p_0$ 为在第 j 个特征分别取值为 $c_1$ 和 $c_0$ 时属于正类的概率</p>
<p>对 OR 取对数
</p>
$$\log(OR) = \log \left( \frac{p_1}{1 - p_1} \right) - \log \left( \frac{p_0}{1 - p_0} \right) = \beta_j (c_1 - c_0)$$<p>那么 OR 可以表示为
</p>
$$OR = e^{\beta_j (c_1 - c_0)}$$<p>如果我们令 $c_1 - c_0 = 1$，那么</p>
$$
OR = e^{\beta_j} = \begin{cases} > 1 & \beta_j > 0 \\ = 1 & \beta_j = 0 \\ < 1 & \beta_j < 0 \end{cases}
$$<p>
三种情况分别对应 危险因子、无作用、保护因子</p>
<h4 id="参数估计">参数估计<a hidden class="anchor" aria-hidden="true" href="#参数估计">#</a></h4>
<p>对于一个实际发生的样本 $i$，它的概率可以表示为
</p>
$$P(y_i | x_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}$$<p>那么似然函数可以表示为
</p>
$$
L = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
$$<p>对似然函数取对数，得到对数似然函数
</p>
$$\begin{aligned}
\log L &= \sum_{i=1}^{n} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right) \\
&= \sum_{i=1}^{n} \left( y_i \log \frac{p_i}{1 - p_i} - \log(1 - p_i) \right) \\
&= \sum_{i=1}^{n} \left( y_i (a x_i + b) - \log(1 + e^{a x_i + b}) \right)
\end{aligned}$$<p>当 $\log L$ 取到最大值时，逻辑回归的参数估计也就完成了</p>
<h4 id="正则化">正则化<a hidden class="anchor" aria-hidden="true" href="#正则化">#</a></h4>
<p>这一部分和正常的机器学习正则化方法类似，主要是为了防止过拟合</p>
<ul>
<li>L1 正则化（<mark>Lasso</mark> 回归）：偏好稀疏编码</li>
<li>L2 正则化（<mark>Ridge</mark> 回归）：偏向于整体更小</li>
</ul>
<h3 id="决策树回归">决策树回归<a hidden class="anchor" aria-hidden="true" href="#决策树回归">#</a></h3>
<p>决策树回归的思想比较简单，就是每次将数据所在的空间一分为二，下面的内容也集中在如何选择最优划分节点上了
划分节点的选择很简单，遍历所有的可能划分点，选择使得划分后的均方误差（MSE）最小的点</p>
<p>很有意思的是，在完成上面一次划分后，可以使用残差 </p>
$$r_i = y_i - \hat{y_i}$$<p> 来继续进行划分，从而得到更好的拟合效果 (提升树)</p>
<blockquote>
<p>决策树回归相当有趣，可以看课本上的具体例子辅助理解</p></blockquote>
<hr>
<h2 id="支持向量机-svm">支持向量机 SVM<a hidden class="anchor" aria-hidden="true" href="#支持向量机-svm">#</a></h2>
<blockquote>
<p>SVM 问题的损失函数叫做 <mark>Hinge</mark> Loss $\to$ $\max(0, 1 - y_i(w^Tx_i + b))$</p></blockquote>
<p><strong>问题推导</strong>：SVM 的目标很简单，就是希望在两个类别之间找到一个最优的划分超平面，先假设数据是线性可分的，而且一个类别为 +1，另一个类别为 -1，那么划分超平面可以表示为
</p>
$$\begin{aligned}
&w \cdot x + b \\
&w \cdot x + b <= -1 \quad for \quad y_i = -1 \\
&w \cdot x + b >= 1 \quad for \quad y_i = +1
\end{aligned}$$<p>结合之前学习过的凸优化知识，可以将 SVM 转化为如下所示的优化问题
</p>
$$\begin{aligned}
&\min \quad \frac{1}{2} ||w||^2 \\
&s.t. \quad y_i (w \cdot x_i + b) >= 1, \quad i = 1, 2, \ldots, n
\end{aligned}$$<p>求解该优化问题可以使用拉格朗日对偶方法，构造拉格朗日函数
</p>
$$L(w, b, \alpha) = \frac{1}{2} ||w||^2 - \sum_{i=1}^{n} \alpha_i [y_i (w \cdot x_i + b) - 1]$$<p>那么求解该优化问题就变成了求解下面的优化问题
</p>
$$\begin{aligned}
&\min_{w,b} \max_{\alpha >= 0} \left( \frac{1}{2} ||w||^2 - \sum_{i=1}^{n} \alpha_i [y_i (w \cdot x_i + b) - 1] \right) \\
= & \max_{\alpha >= 0} \min_{w,b} \left( \frac{1}{2} ||w||^2 - \sum_{i=1}^{n} \alpha_i [y_i (w \cdot x_i + b) - 1] \right) 
\end{aligned}$$<blockquote>
<p>上述交换成立的前提是满足 KKT 条件
</p>
$$\begin{aligned}
&a_i \ge 0 \\
&y_i (w \cdot x_i + b) - 1 \ge 0 \\
&\alpha_i [y_i (w \cdot x_i + b) - 1] = 0, \quad i = 1, 2, \ldots, n
\end{aligned}$$</blockquote>
<p>通过对拉格朗日函数分别对 w 和 b 求偏导，并令其为 0，可以得到
</p>
$$\begin{aligned}
&w = \sum_{i=1}^{n} \alpha_i y_i x_i \\
&\sum_{i=1}^{n} \alpha_i y_i = 0
\end{aligned}$$<p>最终模型可以表示为
</p>
$$f(x) = \text{sign} \left( \sum_{i=1}^{n} \alpha_i y_i x_i^T \cdot x + b \right)$$<p>KKT 条件保证了只有支持向量对应的 $\alpha_i$ 不为 0，因此最终模型只与支持向量有关</p>
<p>当然，SVM 还有软间隔和核函数的内容，不过这里就不展开介绍了 😶</p>
<p>需要知道 Kernel 可以处理非线性可分问题，将数据映射到高维空间</p>
<blockquote>
<p>常见核函数</p></blockquote>
<ul>
<li>高斯 RBF 核函数
$$K(x_i, x_j) = \exp \left( - \frac{||x_i - x_j||^2}{2 \sigma^2} \right)$$</li>
<li>齐次多项式核函数
$$K(x_i, x_j) = (x_i \cdot x_j)^d$$</li>
<li>非齐次多项式核函数
$$K(x_i, x_j) = (x_i \cdot x_j + 1)^d$$</li>
<li>sigmoid 核函数
$$K(x_i, x_j) = \tanh(k x_i \cdot x_j + \theta)$$</li>
</ul>
<hr>
<h2 id="模型的评价">模型的评价<a hidden class="anchor" aria-hidden="true" href="#模型的评价">#</a></h2>
<p>本章一共由三个部分组成：
<a href="#%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87">分类模型评价指标</a> | <a href="#%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98">不平衡分类问题</a> | <a href="#%E8%BF%87%E6%8B%9F%E5%90%88--%E6%AC%A0%E6%8B%9F%E5%90%88">过拟合 &amp; 欠拟合</a></p>
<h3 id="分类模型评价指标">分类模型评价指标<a hidden class="anchor" aria-hidden="true" href="#分类模型评价指标">#</a></h3>
<p>这一节的核心内容应该就是 ROC 曲线，需要知道 FP FN TP TN 的计算、Precision 和 Recall 以及 F1-score 是什么</p>
<p><strong>TP</strong>：将正类正确分类为正类的数量</p>
<p><strong>TN</strong>：将负类正确分类为负类的数量</p>
<p><strong>FP</strong>：将负类错误分类为正类的数量</p>
<p><strong>FN</strong>：将正类错误分类为负类的数量</p>
<p><strong>准确率 (Accuracy)</strong>：分类器正确分类的比例
</p>
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$<p><strong>精确度 (Precision)</strong>：正类预测中真正类所占的比例
</p>
$$Precision = \frac{TP}{TP + FP}$$<blockquote>
<p>需要注意 Precision 和 Accuracy 二者的不同 ⚠️</p></blockquote>
<p><strong>召回率 (Recall)</strong>：正类被正确预测的比例
</p>
$$Recall = \frac{TP}{TP + FN}$$<p><strong>F1-score</strong>：准确率和召回率的调和平均数
</p>
$$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$<h4 id="roc-曲线"><mark>ROC 曲线</mark><a hidden class="anchor" aria-hidden="true" href="#roc-曲线">#</a></h4>
<p>TPR（真正率）和 FPR（假正率）的计算
</p>
$$\begin{aligned}
&TPR = \frac{TP}{TP + FN} \quad (Recall) \\
&FPR = \frac{FP}{FP + TN}
\end{aligned}$$<p>三种常见情况</p>
<ul>
<li>(FPR, TPR) = (0, 0)：将所有样本都预测为负类</li>
<li>(FPR, TPR) = (1, 1)：将所有样本都预测为正类</li>
<li>(FPR, TPR) = (0, 1)：完美分类器</li>
</ul>
<h3 id="不平衡分类问题">不平衡分类问题<a hidden class="anchor" aria-hidden="true" href="#不平衡分类问题">#</a></h3>
<blockquote>
<p>这一块的知识点还蛮重要的，至少在课本后面的题目中经常见到</p></blockquote>
<p>核心目标是解决样本数据集不平衡导致分类器偏向于多数类的问题</p>
<p>第一种方法：<mark>重采样</mark>，包括过采样和欠采样</p>
<ul>
<li>过采样：增加少数类样本数量（可能导致过拟合）</li>
<li>欠采样：减少多数类样本数量（可能训练不充分，但是可以通过多次采样缓解）</li>
</ul>
<p>第二种方法：<mark>两阶段学习</mark>，分为特征学习与分类器学习</p>
<ul>
<li>两阶段学习：PN-Rules
<ul>
<li>基于规则的分类</li>
<li>学习分成两个阶段，每个阶段学习一组规则</li>
</ul>
</li>
<li>训练也包含两个阶段
<ul>
<li>阶段一：学习一组规则，尽可能覆盖正类（少数类）</li>
<li>阶段二：使用阶段一覆盖的正负样本以及少部分为覆盖的负样本，学习一组规则</li>
</ul>
</li>
<li>分类使用两组规则
<ul>
<li>先使用阶段一的规则进行分类，若分类为负类则输出负类</li>
<li>否则使用阶段二的规则进行分类，若分类为正类则输出正类，否则输出负类</li>
</ul>
</li>
</ul>
<p>第三种方法：<mark>代价敏感学习</mark></p>
<ul>
<li>通过调整不同类型分类错误的代价，来缓解不平衡分类问题</li>
<li>代价矩阵</li>
</ul>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Predicted Positive</th>
          <th>Predicted Negative</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Actual Positive</td>
          <td>0</td>
          <td>$C_{FN}$</td>
      </tr>
      <tr>
          <td>Actual Negative</td>
          <td>$C_{FP}$</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>F1-score 在不平衡分类问题中更有意义，它更强调精确率和召回率，但不考虑误分类代价</p></blockquote>
<h3 id="过拟合--欠拟合">过拟合 &amp; 欠拟合<a hidden class="anchor" aria-hidden="true" href="#过拟合--欠拟合">#</a></h3>
<p>过拟合的原因：噪声导致的过拟合、缺乏代表性样本导致的过拟合</p>
<p>减少泛化误差的方法：添加正则化项、Dropout、交叉验证</p>
<table>
  <thead>
      <tr>
          <th>过拟合</th>
          <th>欠拟合</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>低偏差</td>
          <td>高偏差</td>
      </tr>
      <tr>
          <td>高方差</td>
          <td>低方差</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="关联规则挖掘">关联规则挖掘<a hidden class="anchor" aria-hidden="true" href="#关联规则挖掘">#</a></h2>
<p>内容主要包含以下几部分：</p>
<p><a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">基本概念</a> | <a href="#apriori%E7%AE%97%E6%B3%95">Apriori算法</a> | <a href="#fp-growth%E7%AE%97%E6%B3%95">FP-Growth算法</a></p>
<h3 id="基本概念-1">基本概念<a hidden class="anchor" aria-hidden="true" href="#基本概念-1">#</a></h3>
<p>在前面讲述过了规则的定义，那就可以开始学习关联规则挖掘了，如果我们提出假设 $A \to B$，那么我们需要评估这个假设的质量，常用的评估指标有以下三种：
<strong>支持度 (Support)</strong></p>
<p>AB在一起出现的频率
</p>
$$Support(A \to B) = P(A \cup B) = \frac{count(A \cup B)}{N}$$<p><strong>置信度 (Confidence)</strong></p>
<p>B 在 A 发生的条件下发生的概率
</p>
$$Confidence(A \to B) = P(B|A) = \frac{count(A \cup B)}{count(A)}$$<p>
<strong>提升度 (Lift)</strong></p>
<p>衡量规则 $A \to B$ 对 B 发生的影响程度
</p>
$$Lift(A \to B) = \frac{P(B|A)}{P(B)} = \frac{Confidence(A \to B)}{P(B)}$$<blockquote>
<p>❗️ 记住上面三种评估指标的计算公式</p></blockquote>
<h3 id="apriori算法">Apriori算法<a hidden class="anchor" aria-hidden="true" href="#apriori算法">#</a></h3>
<p>⚠️ 候选项集生成的一个重要策略是，在删除最后一个项后，前面的项相同才能合并</p>
<p>‼️ 如果生成的一个项，他的所有子集有不是频繁的情况，需要剪枝删除</p>
<h3 id="fp-growth算法">FP-Growth算法<a hidden class="anchor" aria-hidden="true" href="#fp-growth算法">#</a></h3>
<h2 id="集成学习">集成学习<a hidden class="anchor" aria-hidden="true" href="#集成学习">#</a></h2>
<h3 id="adaboost-算法">AdaBoost 算法<a hidden class="anchor" aria-hidden="true" href="#adaboost-算法">#</a></h3>
<p>不断关注前一轮分类失败的样本，将多个弱学习器组合成一个强学习器</p>
<blockquote>
<p>理论上可以训练出误差任意低的分类器，但是弱分类器要求正确率大于 50%</p></blockquote>
<p>一般损失函数为指数函数</p>
<p><strong>具体流程</strong></p>
<ol>
<li>初始化样本权重
$$
w_i = \frac{1}{N}, \quad i = 1, 2, \ldots, N
$$</li>
<li>对 m = 1, 2, &hellip;, M 重复以下步骤
<ol>
<li>使用加权样本训练弱分类器 $G_m(x)$</li>
<li>计算分类误差率
$$
   err_m = \frac{\sum_{i=1}^{N} w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^{N} w_i}
   $$</li>
<li><mark>计算分类器权重</mark>
$$
   \alpha_m = \frac{1}{2} \log \left( \frac{1 - err_m}{err_m} \right)
   $$</li>
<li>更新样本权重
$$
   w_i \leftarrow w_i \times \exp(-\alpha_m y_i G_m(x_i)), \quad i = 1, 2, \ldots, N$$</li>
<li>归一化样本权重
$$
    w_i \leftarrow \frac{w_i}{\sum_{j=1}^{N} w_j}, \quad i = 1, 2, \ldots, N$$</li>
</ol>
</li>
<li>最终分类器
$$
G(x) = \text{sign} \left( \sum_{m=1}^{M} \alpha_m G_m(x) \right)
$$</li>
</ol>
<p>#TODO</p>
<ul>
<li><input disabled="" type="checkbox"> 算一遍课本上的例题</li>
</ul>
<hr>
<h2 id="错题大赏会">错题大赏会<a hidden class="anchor" aria-hidden="true" href="#错题大赏会">#</a></h2>
<h3 id="精选试卷四">精选试卷四<a hidden class="anchor" aria-hidden="true" href="#精选试卷四">#</a></h3>
<p>1.精选试题四的第三题第二小问，答案给出的对于准确度和召回率的计算非常奇怪，没看明白什么意思</p>
<p><mark>2.</mark>第四题</p>
<ul>
<li>计算距离的时候，对于性别籍贯这种<strong>名目型属性（课本中叫做标称）</strong>，如果值相同则距离为0，不同则距离为1</li>
<li>而在计算点到簇的距离时，对于标称数据，计算的是 1 - P，P 代表该值在簇中出现的频率</li>
<li>计算簇到簇的距离时，举个例子</li>
</ul>
<blockquote>
<p>C1 = {男 ： 25 ， ⼥ ：5 ； ⼴州 ： 20 ， 深圳 ：6 ， 韶关 ：4 ； 20 }
C2 = {男 ： 3 ， ⼥ ： 12 ； 汕 头 ： 12 ， 深 圳 ： 1 ， 韶 关 ： 2 ； 24 }
</p>
$$\begin{aligned}
d =& \frac{(1 - \frac{25}{30} \times \frac{3}{15}) + (1 - \frac{5}{30} \times \frac{12}{15})}{2} \\
&+ \frac{(1 - \frac{20}{30} \times \frac{0}{15}) + (1 - \frac{0}{30} \times \frac{12}{15}) + (1 - \frac{6}{30} \times \frac{1}{15}) + (1 - \frac{4}{30} \times \frac{2}{15})}{4} \\
&+ |20 - 24|
\end{aligned}$$</blockquote>
<h3 id="精选试卷五">精选试卷五<a hidden class="anchor" aria-hidden="true" href="#精选试卷五">#</a></h3>
<p>第一题</p>
<ul>
<li>关联规则挖掘并不是单纯找出所有满足最小支持度的项，还需要保留那些满足最小置信度的规则</li>
<li>K-means 算法是基于原型 or 质心的分类方法，且需要预先指定 K 值</li>
</ul>
<p>第二题</p>
<ul>
<li>计算贝叶斯分类器，实际上就是计算各个类别的后验概率，然后选择概率最大的类别作为预测结果</li>
<li>题目答案给错了，学生栏的数据统计反了</li>
</ul>
<h3 id="精选试卷六">精选试卷六<a hidden class="anchor" aria-hidden="true" href="#精选试卷六">#</a></h3>
<p>单选第七题</p>
<ul>
<li>数据预处理包括：数据清洗、数据集成、数据规约、数据变换（感觉这一题更应该选 A 变量代换）</li>
</ul>
<p>单选第14题</p>
<ul>
<li>对于一个包含 n 个项的频繁项集，可以产生 $2^n - 2$ 条关联规则</li>
</ul>
<p>单选第17题答案错误</p>
<p>多选第一题</p>
<ul>
<li>模式发现不属于预测建模任务，模式匹配更像是数据检索</li>
</ul>
<p>判断第六题</p>
<ul>
<li>特征提取高度依赖特定的领域</li>
</ul>
<p>判断第十四题</p>
<ul>
<li>贝叶斯算法的核心目标是计算后验概率，而且不是依赖全体数据，而是每个类别的条件概率</li>
</ul>
<h3 id="精选试卷七">精选试卷七<a hidden class="anchor" aria-hidden="true" href="#精选试卷七">#</a></h3>
<p>单选第十题</p>
<ul>
<li>极差在取平均的情况下应该是 36，不知道答案中的31从哪里算出来的</li>
</ul>
<p>单选第八题</p>
<ul>
<li>信息熵的计算方法是 $H = -\sum_{i=1}^n p_i \log_2(p_i)$</li>
<li>在等可能情况下简化成 $H = \log_2(n)$</li>
</ul>
<h3 id="精选试卷八">精选试卷八<a hidden class="anchor" aria-hidden="true" href="#精选试卷八">#</a></h3>
<p>选择</p>
<p>第10题</p>
<ul>
<li>当训练集规模较大时，SGD 比批处理梯度下降更优</li>
</ul>
<p>第12题</p>
<ul>
<li>逻辑回归对噪声也是敏感的</li>
</ul>
<p>第十三题</p>
<ul>
<li>若 SVM 的支撑向量过多，则说明训练的模型过拟合</li>
</ul>
<p>多选</p>
<p>第四题</p>
<ul>
<li>对于随机森里的决策树，特征数量 m 越大，单棵树的相关性越大，方差越小；越小则相关性越小但方差变大</li>
</ul>
<p>第十五题</p>
<ul>
<li>属性的别名又叫做维度、特征、字段</li>
</ul>
<h3 id="精选试卷九">精选试卷九<a hidden class="anchor" aria-hidden="true" href="#精选试卷九">#</a></h3>
<p>如果对于贝叶斯分类任务中，误把某一个特征多算了一遍，意味着加强了这个特征对分类的影响</p>
<p>此外如果贝叶斯所有分类都重复一遍，结果可能会发生变化，因为分类的外部概率未知，平方可能会缩小概率差距从而影响分类结果</p>
<p>决策树的父节点的熵比子节点更大</p>
<p>对于 <strong>噪声的敏感度</strong> 排序：</p>
<blockquote>
<p>AdaBoost &gt; 逻辑回归 / 线性 SVM &gt; 软间隔 SVM &gt; 随机森林</p></blockquote>
<p>KNN、线性回归需要对数据进行规范化处理，而贝叶斯分类器和决策树不需要（二者关注相对顺序）</p>
<p>对比 BootStrap 和 K折叠交叉验证</p>
<ul>
<li>当数据量小的时候，BootStrap 更合适</li>
<li>BootStrap 对集成学习有很大帮助</li>
<li>交叉验证一般能提高模型的泛化能力</li>
<li>难以划分训练测试集的时候，BootStrap 更合适</li>
</ul>
<p>虽然决策树也能处理回归问题，但是当需要得到函数表达式时，线性回归更合适</p>
<p>三种可以处理高维数据可视化的方法</p>
<ul>
<li>平行坐标系</li>
<li>散点图矩阵</li>
<li>切尔诺夫脸</li>
</ul>
<p>关于 One Hot 独热编码 🆚 LabelEncoder 标签编码</p>
<ul>
<li>对数值不敏感的模型（如决策树、朴素贝叶斯等），可以使用标签编码</li>
<li>对数值敏感的模型（如线性回归、逻辑回归、KNN、SVM 等），应该使用独热编码</li>
</ul>
<h3 id="精选试卷十">精选试卷十<a hidden class="anchor" aria-hidden="true" href="#精选试卷十">#</a></h3>
<p>SVM 对于类别不平衡的分类问题也能有不错的表现，而贝叶斯、神经网络、KNN 等模型则非常敏感</p>
<p>PCA 的投影方向是数据散度更大的方向，而 LDA 的投影方向才是类别区分度更大的方向（多了一个label的约束）</p>
<p>此外 PCA 并不强调属性值规范化（附加项），常规流程包含 取均值、矩阵特征值分解、坐标变换</p>
<p><strong>Kernel Trick</strong> 的核心思想是通过核函数计算高维空间中数据点的内积，从而避免显式地进行高维映射，降低计算复杂度</p>
<p>计算统计数据的标准差时，一般使用样本标准差公式（分母为 n - 1），而不是总体标准差公式（分母为 n），但是对数据进行 Z-score 规范化时，使用的标准差是总体标准差公式（分母为 n）</p>
<h3 id="精选试卷十一">精选试卷十一<a hidden class="anchor" aria-hidden="true" href="#精选试卷十一">#</a></h3>
<p>当进行 <strong>Lasso</strong> 回归时，如果一个特征被放大了 k &gt; 1 倍，那么它对应的系数会缩小 k 倍，反而是的在进行正则化时，对损失函数的贡献变小，更容易保留该特征</p>
<p>数据质量的指标不包含 精确性(Precision)</p>
<p>书本上给出的<mark>数据质量</mark>的指标是</p>
<ul>
<li>准确性</li>
<li>完整性</li>
<li>一致性</li>
<li>合时性</li>
<li>可信性、可解释性</li>
</ul>
<p>对于<strong>双峰数据</strong>类型，使用离散化方法来处理更合适，而 Min-Max 规范化和 Z-score 规范化更适合单峰数据类型</p>
<p>逻辑回归比决策树更不容易过拟合</p>
<p>对于样本不均衡的分类任务，使用 F1-score、G-mean、AUC 作为评价指标更合适，而 Accuracy 则不合适</p>
<h3 id="精选试卷十二">精选试卷十二<a hidden class="anchor" aria-hidden="true" href="#精选试卷十二">#</a></h3>
<p>对于 逻辑回归 而言，MSE 不是一个合适的损失函数</p>
<p>SVM 与 逻辑回归的核心区别在于 LOSS 函数不同</p>
<hr>
<h2 id="考试回忆录">考试回忆录<a hidden class="anchor" aria-hidden="true" href="#考试回忆录">#</a></h2>
<p>基本全是原题，十二套模拟卷基本完成了测试集的全覆盖</p>
<p>除了有简答题考了 hunt 决策树算法的基本原则和对标称数据的处理方法之外，其余题目基本都是已有的题目，绝大部分都没有修改数据</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://crazyjassbread.github.io/tags/data-mining/">Data Mining</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on x"
            href="https://x.com/intent/tweet/?text=Data%20Mining%20Review&amp;url=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f&amp;hashtags=DataMining">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f&amp;title=Data%20Mining%20Review&amp;summary=Data%20Mining%20Review&amp;source=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f&title=Data%20Mining%20Review">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on whatsapp"
            href="https://api.whatsapp.com/send?text=Data%20Mining%20Review%20-%20https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on telegram"
            href="https://telegram.me/share/url?text=Data%20Mining%20Review&amp;url=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Data Mining Review on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Data%20Mining%20Review&u=https%3a%2f%2fcrazyjassbread.github.io%2fposts%2ffinal-exam-review%2fdata-mining%2freview%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://crazyjassbread.github.io/">CrazyBread&#39; Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
